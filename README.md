# ğŸ§  Anti-Money Laundering Detection System (Big Data Project)

This project implements a complete Big Data pipeline for detecting suspicious financial transactions using PySpark, Spark MLlib, XGBoost, and Streamlit. The system is designed to handle and process large-scale transactional datasets efficiently, apply machine learning for risk prediction, and offer an intuitive UI for both real-time and batch evaluation.

---

## ğŸ“Š Project Highlights

- **Big Data Processing:** Built using **Apache Spark (PySpark)** to scale feature engineering and labeling on millions of transactions.
- **Machine Learning Models:** Trained **GBTClassifier (MLlib)** and **XGBoost** models using engineered features.
- **Real-Time Scoring:** A user-facing **Streamlit app** allows both single transaction and batch (2GB+) scoring.
- **Batch Risk Assessment:** Large-scale transaction CSVs can be uploaded and scored with live feedback.
- **Deployment-Ready:** Includes `requirements.txt`, GitHub integration, and clean modular code for real-world deployment.

---

## ğŸ› ï¸ Tech Stack

| Layer              | Tools / Libraries                            |
|--------------------|-----------------------------------------------|
| **Data Processing**| Apache Spark (PySpark)                        |
| **Feature Pipeline**| Spark SQL, window functions                  |
| **Model Training** | Spark MLlib (GBT), XGBoost                    |
| **Web UI**         | Streamlit                                     |
| **Model Management** | joblib, scikit-learn                        |
| **Version Control**| Git, GitHub                                   |
| **Data Storage**   | CSV files (2GB+ handled)                      |

---


## ğŸ—‚ï¸ Project Structure
```


â”œâ”€â”€ app.py # Streamlit app
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ code/
â”‚ â”œâ”€â”€ feature_engineering_pipeline.py
â”‚ â”œâ”€â”€ extract_laundering_transactions.py
â”‚ â”œâ”€â”€ xgb_aml_model.pkl
â”‚ â”œâ”€â”€ gbt_aml_model.pkl
â”‚ â””â”€â”€ HI-Small_Trans.csv # Sample input
â”œâ”€â”€ data/
â”‚ â””â”€â”€ final_ml_dataset.csv # Engineered features
â””â”€â”€ docs/
â””â”€â”€ architecture_diagram.png
```
---

## ğŸ§± Big Data Workflow Overview

### 1ï¸âƒ£ Data Ingestion
- Source: IBM AML Dataset (HI-Small)
- Ingested via `SparkSession.read.csv()` with schema inference

### 2ï¸âƒ£ Feature Engineering (PySpark)
- Grouped and aggregated by `From_Account`
- Features: `TxnCount`, `AvgAmount`, `StdDevAmount`, `HourBucket`, etc.
- Timestamp parsing and hour binning (`percentile_approx`)

### 3ï¸âƒ£ Label Extraction
- Extract laundering patterns from semi-structured logs (`.txt`) using regex
- Join labeled transactions to feature matrix

### 4ï¸âƒ£ Model Training
- Trained on Spark-generated features
- Models: `GBTClassifier` (Spark MLlib) and `XGBoostClassifier` (via joblib)

### 5ï¸âƒ£ Streamlit App
- Real-time prediction: manual input form
- Batch prediction: handle 2GB+ CSVs with live progress bar
- User-selectable model for demonstration and comparison

---

## ğŸš€ How to Run

### 1. Set up the environment

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

```

2. Run the Spark pipelines
  
3. Launch the Streamlit app
```bash
streamlit run app.py
```

ğŸ§  Contributors
Shashidhar Babu P V D

Vaheedur Rehman Mahmud

Abhinav Vummidichetty

Suharsha
